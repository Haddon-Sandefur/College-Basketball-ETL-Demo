from bs4 import BeautifulSoup
#import numpy as np 
import pandas as pd
import requests
import os
import json
#from lxml import html
import time
from tqdm import tqdm
from global_ import priorDataBoolean

#Put path here
#os.chdir(r'/Users/hadsa/OneDrive/Documents/GitHub/CBB_Demo')

with open("cbb_Links.json", "r") as f:
    links = json.load(f)
    
    
team_stats = []
team_lines = []
team_score = []
full_data =  []



# Define a function to perform row-wise operations that assign the appropriate spread to the favored/unfavored team
        
def spreadassign(row):
    if row['Teams'] == row["Favored Team"]: # Favored Team Spread Assignment
        value = float(spread.split()[2]) # This is the line
    else:
        value = -float(spread.split()[2])
    return value

# Our text file of links is continuously appended, so we have to keep count of the number of rows we run each time we batch this script.
# This is so we don't have to re-scrape every webpage, and just start where we left off on the previous row of the links text file
 
# Read the length (number of rows) of the old links text file conditional on priorRunBoolean

if priorDataBoolean:
    with open("last_run_count.txt", "r") as f:
        lastRowRan = int(f.read())
    print("Prior data identified in path")
else:
   lastRowRan = 0
   print("No data identified in path. Is this the start of the season?")
    


# ETL process...
for link in tqdm(links[lastRowRan:]):
    response = requests.get(link, timeout = 60)
    
    try:
    
        soup = BeautifulSoup(response.text, "html.parser") #Capture webpage with Soup
        spread = soup.find("div", class_='n8 GameInfo__BettingItem flex-expand line').text
        
        alt_name= soup.find_all("a", class_="AnchorLink truncate") # Let's get non-abbreviated names for each team. this will make our life easier in the future
        alt_name_list = []
        
        for name in alt_name:
            alt_name_list.append(name.text)
        
        #tree =  html.fromstring(response.text)
        #spread = tree.xpath('//*[@id="fittPageContainer"]/div[2]/div/div[5]/div/div/section[2]/div/div[1]/div[3]/div[1]/div[1]').text
        
        dfs = pd.read_html(link) # Get data tables and turn them into pds
        
        # Grab Team Names
        df_teams = dfs[0]
        df_teams.rename(columns = {'Unnamed: 0':"Teams"}, inplace = True) # Rename the unamed column with "Teams"

        # Put the stats found on ESPN's matchup page in wide format and then edit
        df_stats = dfs[1].T # Pivot to Wide Format
        df_stats.columns = df_stats.iloc[0] # Make the first row the column header
        df_stats = df_stats[1:] # Delete Extra row generated by the above two lines
        df_stats.reset_index(drop=True, inplace=True) # Reset Index
        df_stats.drop(["FG", "FT", "3PT"], axis=1, inplace=True) # Drop Superfluos Variables (We already have them)
        df_stats = df_stats.astype(float) # Changing numbers that are currently string data types to floats
  
      
       
        
        # Some additional wrangling here to get row differences.
        # First, we take df_stats and the boxscores and concatenate them, overwriting to df_stats
        df_stats = pd.concat([df_teams.drop('Teams', axis = 1), # Drop Team Name
                              df_stats], axis=1)
        
        # Bug fix - if a new run starts with a team going into overtime, it incorrectly breaks the appending
        # of the dataframe with prior data. We need to push the overtime variable
        # to the end of df_stats to fix this. Tempcol will reindex later.
        if 'OT' in df_stats:
            tempcol = df_stats.pop('OT')
        else:
            tempcol = float('nan')
        
        # Take the team name from the box score and make the name column its own Dataframe. 
        # Note df_teams["..."] forces a Series datatype so we have to bring it back to a frame
        df_teams = df_teams["Teams"].to_frame() 
        
        
   
        
        
        # Now take the differences of the df_stats rows then replace the df_stats dataframe with these row differences
        row1 = df_stats.iloc[0] - df_stats.iloc[1] # Row 1 - Row 2
        row2 = df_stats.iloc[1] - df_stats.iloc[0] # Row 2 - Row 1
        
        # Overwrite df_stats with the above rows, then transpose
        df_stats = pd.concat([row1, row2], axis=1).T 
        

        df_full = df_teams.join(df_stats) # Merge team Names back to the new differences data
        df_full["Line"] = spread if spread else "NA" # add the line listed on ESPN's webpage to the dataframe
        df_full["Favored Team"] = spread.split()[1] if spread else "NA"
        df_full["Spread"] = df_full.apply(spreadassign, axis = 1) if spread else "NA"
        df_full["Teams Alt"] = alt_name_list
        df_full["OT"] = tempcol
        
        
    except:
        # We don't want D3/D3 or canceled/postponed game data.
        # They are either irrelevant or cause problems, so we make the exception here for them.
        df_full = None
        print(f"Data Not Found/Irrelevant for {link}")
    
    # Appending
    full_data.append(df_full)
    
    # Sleep to prevent overload of ESPN's servers.
    time.sleep(.1)

# Combine into one DataFrame
master_set = pd.concat(full_data, axis=0)

# Save the length (number of rows) of the current links text file for the next run
if priorDataBoolean:
    #Set Header = True for the first run of the season, False o/w
    master_set.to_csv('cbb_DataRaw.txt', sep = '\t', header = False, index = False, mode = 'a')
    print("Prior Data found, appending new data to existing dataset.")
else:
    master_set.to_csv('cbb_DataRaw.txt', sep = '\t', header = True, index = False, mode = 'a')
    print("Creating new dataset.")



